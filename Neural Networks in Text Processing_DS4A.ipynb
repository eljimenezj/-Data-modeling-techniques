{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to prevent overfitting in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "try:\n",
    "    tf.set_random_seed(1337)                    # set the random seed for reproducibility\n",
    "except:\n",
    "    tf.random.set_seed(1337)                     # NOTE: Newer version of tensorflow uses tf.random.set_seed\n",
    "np.random.seed(1337)                         #       instead of tf.set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Business Context.** You are a data scientist working for a machine learning consultancy. One of your clients wants to be able to classify text reviews automatically by the likely rating (on a 1 - 5 scale) that that person would give. However, they do not have sufficient data they generated on their own to do this, so you need to use an external, rich dataset as a basis on which to build your model and then translate it over.\n",
    "\n",
    "**Business Problem.** Your task is to **build a neural networks-based model for classifying text reviews into likely ratings (on a 1 - 5 scale)**.\n",
    "\n",
    "**Analytical Context.** We'll use the Amazon review dataset again and try to classify reviews into star ratings automatically. Instead of just positive and negative, we'll take on the harder challenge of predicting the *exact* star rating. The lowest score is 1 and the highest is 5.\n",
    "\n",
    "Instead of trying to optimize by pre-processing the text, we'll do very basic tokenization and experiment with different neural network models, architectures, and hyperparameters to optimize the results. You'll start by building a simple dense neural network and try to get it to perform better using various techniques. Then you'll evaluate the results and diagnose where it tends to perform more poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and preparing the data\n",
    "\n",
    "We'll mainly be using the `keras` module from TensorFlow, but we'll also use `pandas` to read the CSV file and `sklearn` for some helper functions. We'll be using only the \"Text\" and \"Score\" columns in the `Reviews.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews = pd.read_csv('Reviews.csv', nrows=262084)\n",
    "amazon_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 1:\n",
    "\n",
    "\n",
    "Combine the first 1,000 of each of the 1-, 2-, 3-, 4-, and 5-star reviews in `amazon_reviews` into a single DataFrame (so you should have 5,000 observations in total). Split this DataFrame into training and test sets, with 80% of the data for the training set.\n",
    "\n",
    "**Hint:** `keras` will expected your labels to start with 0, and not 1, so make sure to adjust the labels accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "for i in range(1,6):\n",
    "    temp = []\n",
    "    temp = amazon_reviews[amazon_reviews.Score ==i ].sample(1000)\n",
    "    appended_data.append(temp)\n",
    "    \n",
    "df = pd.concat(appended_data,  ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>193397</td>\n",
       "      <td>B001PLIGB8</td>\n",
       "      <td>A3PZVEJ94ZG43</td>\n",
       "      <td>Mother005</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1287100800</td>\n",
       "      <td>Broken Dreams</td>\n",
       "      <td>I am not impressed with MYOFFICEPRODUCTS.COM. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>658</td>\n",
       "      <td>202599</td>\n",
       "      <td>B004XXXK5W</td>\n",
       "      <td>A1QDQD4HJKCFI9</td>\n",
       "      <td>Geoffrey J Graham</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1313712000</td>\n",
       "      <td>It's only 75% Juice...it has added sugar and s...</td>\n",
       "      <td>&lt;span class=\"tiny\"&gt; Length:: 0:51 Mins&lt;br /&gt;&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>659</td>\n",
       "      <td>172718</td>\n",
       "      <td>B0002DHOWW</td>\n",
       "      <td>A5NQFXER5QYMD</td>\n",
       "      <td>jay sellers \"jay bird\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1336176000</td>\n",
       "      <td>cats hate it</td>\n",
       "      <td>I couldn't get my cats to eat this if it were ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      Id   ProductId          UserId             ProfileName  \\\n",
       "0      0  193397  B001PLIGB8   A3PZVEJ94ZG43               Mother005   \n",
       "1    658  202599  B004XXXK5W  A1QDQD4HJKCFI9       Geoffrey J Graham   \n",
       "2    659  172718  B0002DHOWW   A5NQFXER5QYMD  jay sellers \"jay bird\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     6                       6      0  1287100800   \n",
       "1                    13                      16      0  1313712000   \n",
       "2                     0                       0      0  1336176000   \n",
       "\n",
       "                                             Summary  \\\n",
       "0                                      Broken Dreams   \n",
       "1  It's only 75% Juice...it has added sugar and s...   \n",
       "2                                       cats hate it   \n",
       "\n",
       "                                                Text  \n",
       "0  I am not impressed with MYOFFICEPRODUCTS.COM. ...  \n",
       "1  <span class=\"tiny\"> Length:: 0:51 Mins<br /><b...  \n",
       "2  I couldn't get my cats to eat this if it were ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].replace({1: 0, \n",
    "                     2: 1, \n",
    "                     3:2, \n",
    "                     4:3, \n",
    "                     5:4}, inplace=True)\n",
    "\n",
    "df.sort_values(by='Score', ascending=True).reset_index().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition\n",
    "\n",
    "train, test = train_test_split(df,\n",
    "                               test_size=0.2,\n",
    "                               random_state=42,\n",
    "                               stratify = df['Score']\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing our texts\n",
    "\n",
    "Keras comes with its own functions to preprocess text, including a [tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) (a mapping from each word in our corpus to a unique integer). Unlike the `CountVectorizer` from `sklearn`, which produces sparse matrices, `keras` often expects to work with sequences representing only the words that occur in a text. To prepare text before feeding it into a neural network, we usually:\n",
    "\n",
    "1. Create a [tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "2. [Create sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) from our text (each text becomes a list of integers, based on the tokenizer mapping, instead of words)\n",
    "3. [Pad or truncate](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) each sequence to a fixed length (very short texts get `0`s added to them, while very long ones are truncated).\n",
    "\n",
    "The tokenizer has a configurable word cap, so it will only consider the $n$ most common words in the corpus, ignoring very rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "30_min"
    ]
   },
   "source": [
    "### Exercise 2:\n",
    "\n",
    "In this exercise, you will learn how to use the `tf.keras.preprocessing.text.Tokenizer` tool to carry out the preprocessing steps described above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "#### 2.1\n",
    "\n",
    "Perform some exploratory analysis of the dataset to calculate the number of unique words in our corpus and the distribution of the number of words in each review of the training set. What is the 80th percentile of this distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to count the words\n",
    "\n",
    "def count_words(corpus):\n",
    "    x = len(corpus.lower().split())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1174    140\n",
       " 4786     23\n",
       " 4915     93\n",
       " 3234    145\n",
       " 852      99\n",
       "        ... \n",
       " 744     100\n",
       " 165     122\n",
       " 1964     66\n",
       " 3479     44\n",
       " 6        60\n",
       " Name: Text, Length: 4000, dtype: int64,\n",
       " 1553     89\n",
       " 3503     23\n",
       " 413      24\n",
       " 2881     51\n",
       " 2618    234\n",
       "        ... \n",
       " 1615     84\n",
       " 561      26\n",
       " 612      81\n",
       " 2714    192\n",
       " 3914     23\n",
       " Name: Text, Length: 1000, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Text'].apply(count_words),  test['Text'].apply(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentile 80th in train dataset is: 129.0\n"
     ]
    }
   ],
   "source": [
    "print('The percentile 80th in train dataset is:', train['Text'].apply(count_words).quantile(.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to explore the most frequent words for each score in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def common_words_ngrams(corpus, n=5,k=1):\n",
    "    # Train countvectorizer with input corpus and personalizate n grams \n",
    "    # Here the stopwords are remove\n",
    "    vec = CountVectorizer(ngram_range=(k,k),                          \n",
    "                          stop_words = 'english').fit(corpus)\n",
    "    \n",
    "    # Creation of bag of words from all corpus\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # Count how many times the ngram appears\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    \n",
    "    # order from most to least occurrences\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the number n (personalizate in input) of most frequents ngrams\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 1360), ('like', 485), ('product', 463), ('just', 354), ('taste', 354)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_ngrams(df[df['Score']==0]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 1368), ('like', 645), ('taste', 485), ('coffee', 376), ('just', 367)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_ngrams(df[df['Score']==1]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 1498), ('like', 654), ('coffee', 472), ('good', 468), ('taste', 466)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_ngrams(df[df['Score']==2]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 1357), ('like', 564), ('coffee', 541), ('good', 538), ('flavor', 366)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_ngrams(df[df['Score']==3]['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 1171), ('like', 380), ('good', 373), ('great', 359), ('just', 294)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_ngrams(df[df['Score']==4]['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "#### 2.2\n",
    "\n",
    "Given the results above, we create a tokenizer using only the top 20,000 most frequent words in our corpus (which corresponds to roughly 80% of the words): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000) #We create the tokenizer using only top 20000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['Text'])  #Then, we create the text->indices mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line has given several features and methods to our tokenizer. For instance, print the line `tokenizer.word_index` in a new cell - what do you see? Apply the `tokenizer.texts_to_sequences()` method on the list `['I just feel very very good']`. Apply the `tokenizer.sequences_to_texts()` method on the list `[[109, 19, 824, 76, 114, 6315, 1137, 8070]]`. What were your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 35, 271, 39, 39, 30]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['I just feel very very good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['box have fair your drink whites prices unsuitable']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[109, 19, 824, 76, 114, 6315, 1137, 8070]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "#### 2.3\n",
    "\n",
    "Use the tokenizer to transform the texts in our test and train data to sequences. Then, use the `pad_sequences` function to pad/truncate these sequences to length 116 (the 80th percentile of text lengths). Save the resulting arrays as `train_sequences` and `test_sequences`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_sequences = tokenizer.texts_to_sequences(train['Text'])\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=116)\n",
    "\n",
    "# Test dataset\n",
    "test_sequences = tokenizer.texts_to_sequences(test['Text'])\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=116)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['Score']\n",
    "labels = labels.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic neural network model \n",
    "\n",
    "Now that we have preprocessed the text, let's create a basic neural network to train on our data. We'll use an embedding layer which performs [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) on our word sequences, two fully connected (\"dense\") layers, and an output layer with 5 neurons to represent the 5 possible star ratings.\n",
    "\n",
    "Before we train a `keras` model, there is an additional `compile` step where we define what loss function and optimizer to use, and what metrics to output. Then we can train the model using the `fit` function. All of this is shown below.\n",
    "\n",
    "Note the `validation_split=0.2` argument which tells Keras to train on only 80% of the training data and tune the model on the remaining 20%, which we call the validation set. You can see the accuracy and loss for both the training and validation set in the output for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 116, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 116, 128)          16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 116, 128)          16512     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,593,669\n",
      "Trainable params: 2,593,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 1.5917 - accuracy: 0.2609 - val_loss: 1.5470 - val_accuracy: 0.3750\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 1.4117 - accuracy: 0.4056 - val_loss: 1.3573 - val_accuracy: 0.3925\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 1.0930 - accuracy: 0.5678 - val_loss: 1.3520 - val_accuracy: 0.4400\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.7905 - accuracy: 0.7188 - val_loss: 1.4607 - val_accuracy: 0.4212\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.4827 - accuracy: 0.8628 - val_loss: 1.6809 - val_accuracy: 0.4075\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.2620 - accuracy: 0.9428 - val_loss: 1.8573 - val_accuracy: 0.4125\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.1292 - accuracy: 0.9812 - val_loss: 2.0611 - val_accuracy: 0.4100\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.0615 - accuracy: 0.9956 - val_loss: 2.2408 - val_accuracy: 0.3950\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.0323 - accuracy: 0.9987 - val_loss: 2.3469 - val_accuracy: 0.3975\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 2.4337 - val_accuracy: 0.4013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be5da768c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "### Exercise 3:\n",
    "\n",
    "How well does this model perform? How does this compare to a baseline expectation? What do you notice about the accuracy and loss values for both the validation and training sets over time and what does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that throughout the epochs the performance of the model does not improve very well the accuracy metric in validation, while in training it learns the data completely. Moreover, the value of the cost function in training is decreasing while in validation it does not improve. We should try different strategies later on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different regularization strategies\n",
    "\n",
    "There are many different ways to mitigate overfitting in a neural network, collectively known as *regularization* techniques. One common regularization technique is called [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout). In this regularization method, a set of neurons is randomly selected at each training step to be completely ignored. This is done so that the neurons in our network do not rely strongly on their neighboring neurons and we avoid the creation of [\"co-adaptations\"](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) that do not generalize well to unseen data. This making the model more robust and less prone to overffiting.\n",
    "\n",
    "You can create dropouts in `keras` by adding a layer named `Dropout(p)`, where `p` is the probability of dropping neurons in the previous layer. For example, the following model would implement dropout by removing roughly 20% percent of the outputs of the embedding layer at each training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 1.6030 - accuracy: 0.2322 - val_loss: 1.5801 - val_accuracy: 0.3375\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 1.4823 - accuracy: 0.3837 - val_loss: 1.4194 - val_accuracy: 0.4025\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 1.2309 - accuracy: 0.4891 - val_loss: 1.3637 - val_accuracy: 0.4238\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.9999 - accuracy: 0.5931 - val_loss: 1.4373 - val_accuracy: 0.3875\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.7464 - accuracy: 0.7303 - val_loss: 1.5665 - val_accuracy: 0.4087\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.5186 - accuracy: 0.8341 - val_loss: 1.7749 - val_accuracy: 0.4150\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3397 - accuracy: 0.9075 - val_loss: 1.9558 - val_accuracy: 0.4175\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.2096 - accuracy: 0.9456 - val_loss: 2.1774 - val_accuracy: 0.4025\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.1185 - accuracy: 0.9794 - val_loss: 2.4619 - val_accuracy: 0.3900\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.0707 - accuracy: 0.9903 - val_loss: 2.5478 - val_accuracy: 0.3963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be5dae9a08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Modify the neural network definition above to try and fix the overfitting problem using Dropout. Explain the configuration that you tried and your results. Why do you think your modifications were or were not able to mitigate the overfitting problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 1.6079 - accuracy: 0.2100 - val_loss: 1.5996 - val_accuracy: 0.2862\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 1.5730 - accuracy: 0.2825 - val_loss: 1.5557 - val_accuracy: 0.3200\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 1.4541 - accuracy: 0.3644 - val_loss: 1.4388 - val_accuracy: 0.3812\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 1.3102 - accuracy: 0.4328 - val_loss: 1.3931 - val_accuracy: 0.3887\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 5s 45ms/step - loss: 1.1715 - accuracy: 0.4991 - val_loss: 1.3677 - val_accuracy: 0.3963\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 1.0332 - accuracy: 0.5691 - val_loss: 1.3810 - val_accuracy: 0.3988\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.9077 - accuracy: 0.6359 - val_loss: 1.4088 - val_accuracy: 0.3988\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.7526 - accuracy: 0.7050 - val_loss: 1.4995 - val_accuracy: 0.3600\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.5924 - accuracy: 0.7713 - val_loss: 1.6164 - val_accuracy: 0.3550\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.4549 - accuracy: 0.8316 - val_loss: 1.7912 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be5edc8108>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx4 = Sequential()\n",
    "model_ecx4.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx4.add(Dense(128, activation='relu'))\n",
    "model_ecx4.add(Dropout(0.5))\n",
    "model_ecx4.add(Dense(128, activation='relu'))\n",
    "model_ecx4.add(Dropout(0.5))\n",
    "model_ecx4.add(GlobalMaxPooling1D())\n",
    "model_ecx4.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx4.compile(loss='sparse_categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model_ecx4.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "30_min"
    ]
   },
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Keras allows you to add [L1](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l1), [L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l2), or [L1 and L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l1_l2) combined regularizers on individual layers by passing in the `kernel_regularizer`, `bias_regularizer` or `activity_regularizer` arguments. In neural networks, these regularizers work by penalizing the loss function in different ways, based on the number of weights or the size of the weights.\n",
    "\n",
    "Try 4-5 different combinations of L1, L2, L1 and L2 regularization in different combinations on different layers. In each example, explain why you tried that configuration and the results. Why do you think your modifications were or were not able to mitigate the overfitting problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 2.8667 - accuracy: 0.2066 - val_loss: 1.9374 - val_accuracy: 0.1863\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6788 - accuracy: 0.1894 - val_loss: 1.6142 - val_accuracy: 0.1850\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6135 - accuracy: 0.1903 - val_loss: 1.6143 - val_accuracy: 0.1850\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6134 - accuracy: 0.2025 - val_loss: 1.6144 - val_accuracy: 0.1863\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6134 - accuracy: 0.2019 - val_loss: 1.6143 - val_accuracy: 0.1850\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 1.6134 - accuracy: 0.2037 - val_loss: 1.6146 - val_accuracy: 0.1850\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 1.6134 - accuracy: 0.2037 - val_loss: 1.6146 - val_accuracy: 0.1850\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6133 - accuracy: 0.2037 - val_loss: 1.6145 - val_accuracy: 0.1850\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6133 - accuracy: 0.1975 - val_loss: 1.6145 - val_accuracy: 0.1863\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6133 - accuracy: 0.2019 - val_loss: 1.6146 - val_accuracy: 0.1850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be5f43ba48>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx5_1 = Sequential()\n",
    "model_ecx5_1.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx5_1.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_ecx5_1.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_ecx5_1.add(GlobalMaxPooling1D())\n",
    "model_ecx5_1.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx5_1.compile(loss='sparse_categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model_ecx5_1.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 1.7820 - accuracy: 0.2378 - val_loss: 1.7002 - val_accuracy: 0.3413\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6189 - accuracy: 0.3234 - val_loss: 1.5495 - val_accuracy: 0.3525\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.4510 - accuracy: 0.3988 - val_loss: 1.4956 - val_accuracy: 0.3988\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.2879 - accuracy: 0.4828 - val_loss: 1.5133 - val_accuracy: 0.3925\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 1.1182 - accuracy: 0.5806 - val_loss: 1.5854 - val_accuracy: 0.4100\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.9711 - accuracy: 0.6509 - val_loss: 1.6992 - val_accuracy: 0.3963\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.8339 - accuracy: 0.7297 - val_loss: 1.8416 - val_accuracy: 0.3963\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.7152 - accuracy: 0.7844 - val_loss: 2.0640 - val_accuracy: 0.3700\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.6024 - accuracy: 0.8350 - val_loss: 2.2045 - val_accuracy: 0.3738\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.5174 - accuracy: 0.8681 - val_loss: 2.3688 - val_accuracy: 0.3762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be62da2188>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx5_2 = Sequential()\n",
    "model_ecx5_2.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx5_2.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "model_ecx5_2.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "model_ecx5_2.add(GlobalMaxPooling1D())\n",
    "model_ecx5_2.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx5_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model_ecx5_2.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 1.7370 - accuracy: 0.2244 - val_loss: 1.6492 - val_accuracy: 0.2812\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 1.5826 - accuracy: 0.3156 - val_loss: 1.5146 - val_accuracy: 0.3187\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.4324 - accuracy: 0.3791 - val_loss: 1.4486 - val_accuracy: 0.3787\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.3085 - accuracy: 0.4578 - val_loss: 1.4778 - val_accuracy: 0.3825\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.1485 - accuracy: 0.5534 - val_loss: 1.5510 - val_accuracy: 0.3887\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.9949 - accuracy: 0.6300 - val_loss: 1.6851 - val_accuracy: 0.3862\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.8484 - accuracy: 0.7050 - val_loss: 1.8324 - val_accuracy: 0.3750\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.7322 - accuracy: 0.7603 - val_loss: 1.9801 - val_accuracy: 0.3700\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.6387 - accuracy: 0.8103 - val_loss: 2.2410 - val_accuracy: 0.3550\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.5667 - accuracy: 0.8431 - val_loss: 2.3525 - val_accuracy: 0.3438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be6320ed08>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx5_3 = Sequential()\n",
    "model_ecx5_3.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx5_3.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_ecx5_3.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_ecx5_3.add(GlobalMaxPooling1D())\n",
    "model_ecx5_3.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx5_3.compile(loss='sparse_categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model_ecx5_3.fit(train_sequences, labels, validation_split=0.2, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 2.6923 - accuracy: 0.2250 - val_loss: 1.8702 - val_accuracy: 0.1863\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6990 - accuracy: 0.2434 - val_loss: 1.6243 - val_accuracy: 0.2100\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6137 - accuracy: 0.1925 - val_loss: 1.6112 - val_accuracy: 0.1850\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6097 - accuracy: 0.2025 - val_loss: 1.6107 - val_accuracy: 0.1863\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 1.6097 - accuracy: 0.2019 - val_loss: 1.6105 - val_accuracy: 0.1850\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6096 - accuracy: 0.2037 - val_loss: 1.6110 - val_accuracy: 0.1850\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 1.6096 - accuracy: 0.2037 - val_loss: 1.6107 - val_accuracy: 0.1850\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 1.6095 - accuracy: 0.1922 - val_loss: 1.6106 - val_accuracy: 0.1850\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6095 - accuracy: 0.2016 - val_loss: 1.6107 - val_accuracy: 0.1863\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 1.6094 - accuracy: 0.1994 - val_loss: 1.6108 - val_accuracy: 0.1850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be00b9e088>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx5_4 = Sequential()\n",
    "model_ecx5_4.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx5_4.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(0.00001)))\n",
    "model_ecx5_4.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(0.00001)))\n",
    "model_ecx5_4.add(GlobalMaxPooling1D())\n",
    "model_ecx5_4.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx5_4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ecx5_4.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization through adding more data\n",
    "\n",
    "Depending on the configurations you tried above, you probably saw that L1 and L2 regularization are pretty limited for this model and this amount of data. A more straightforward way to prevent overfitting is simply by adding more training data. If the network has more (and more varied) examples to learn from, perhaps it will learn more generalizable rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "25_min"
    ]
   },
   "source": [
    "### Exercise 6:\n",
    "\n",
    "How would you test the hypothesis that adding more data would result in a more generalizable model? Explain any change in results you see from further experimentation.\n",
    "\n",
    "**Hint:** Try adding 6000 reviews for each score instead. Compare with the original proposed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>121435</td>\n",
       "      <td>B000V6DW5S</td>\n",
       "      <td>A1SC0PCDCLY8R4</td>\n",
       "      <td>pen name \"ok?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1330646400</td>\n",
       "      <td>Gross</td>\n",
       "      <td>I love Italian wedding and Campbell's own regu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4005</td>\n",
       "      <td>252584</td>\n",
       "      <td>B001EQ5ERI</td>\n",
       "      <td>A30DO3OIRLDC8B</td>\n",
       "      <td>Law</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1336953600</td>\n",
       "      <td>Don't bother</td>\n",
       "      <td>I was seriously disappointed with this product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4004</td>\n",
       "      <td>196961</td>\n",
       "      <td>B000EMQFY4</td>\n",
       "      <td>A2J57VGDETZKF6</td>\n",
       "      <td>4 Kids 2 Exhausted</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1327449600</td>\n",
       "      <td>Terribly false advertising</td>\n",
       "      <td>I had my hopes for this bar. The box reads \"Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4003</td>\n",
       "      <td>49909</td>\n",
       "      <td>B00430B73W</td>\n",
       "      <td>A1HCIYQF7NYKE</td>\n",
       "      <td>K. Swanson</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1154995200</td>\n",
       "      <td>Just plain awful</td>\n",
       "      <td>I can at least tolerate most foods, but these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4002</td>\n",
       "      <td>12459</td>\n",
       "      <td>B0079YD36I</td>\n",
       "      <td>A92JJZ71TKRSJ</td>\n",
       "      <td>Leeza</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1340582400</td>\n",
       "      <td>Coffee Lovers Beware</td>\n",
       "      <td>I purchased this coffee on sale at my local Vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>25994</td>\n",
       "      <td>198561</td>\n",
       "      <td>B000FVBYCW</td>\n",
       "      <td>A2BS1XZLSI5FTK</td>\n",
       "      <td>Lulu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1350691200</td>\n",
       "      <td>Great Tea, Great Price</td>\n",
       "      <td>Tea comes in bags inside a large foil lined po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>25993</td>\n",
       "      <td>156887</td>\n",
       "      <td>B000BZ1OUO</td>\n",
       "      <td>A2VJKSJQO9IKNW</td>\n",
       "      <td>D. Newray \"Dazeedave\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1231891200</td>\n",
       "      <td>This is truly the Best Giardiniera</td>\n",
       "      <td>If you can't take spicey, then don't buy the H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>25992</td>\n",
       "      <td>59609</td>\n",
       "      <td>B000W5SLB8</td>\n",
       "      <td>A1ILH94WP2KTA0</td>\n",
       "      <td>Stephen J. Duffey</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1308441600</td>\n",
       "      <td>Wonderful dog food</td>\n",
       "      <td>I used to buy my dog (5 year old Corgi) the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>26001</td>\n",
       "      <td>103538</td>\n",
       "      <td>B002Z08RIA</td>\n",
       "      <td>A39PGI6IGM5Y2A</td>\n",
       "      <td>Lee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>Excellent coconut juice</td>\n",
       "      <td>I'm an avid coconut water drinker - I've tried...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>80909</td>\n",
       "      <td>B0029JW9JU</td>\n",
       "      <td>A26ZK8QFWXN859</td>\n",
       "      <td>dalepres \"dalepres\"</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1301184000</td>\n",
       "      <td>I've eaten these for 15 years</td>\n",
       "      <td>I have been eating these pepperonis for 15 yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      Id   ProductId          UserId            ProfileName  \\\n",
       "0          0  121435  B000V6DW5S  A1SC0PCDCLY8R4         pen name \"ok?\"   \n",
       "1       4005  252584  B001EQ5ERI  A30DO3OIRLDC8B                    Law   \n",
       "2       4004  196961  B000EMQFY4  A2J57VGDETZKF6     4 Kids 2 Exhausted   \n",
       "3       4003   49909  B00430B73W   A1HCIYQF7NYKE             K. Swanson   \n",
       "4       4002   12459  B0079YD36I   A92JJZ71TKRSJ                  Leeza   \n",
       "...      ...     ...         ...             ...                    ...   \n",
       "29995  25994  198561  B000FVBYCW  A2BS1XZLSI5FTK                   Lulu   \n",
       "29996  25993  156887  B000BZ1OUO  A2VJKSJQO9IKNW  D. Newray \"Dazeedave\"   \n",
       "29997  25992   59609  B000W5SLB8  A1ILH94WP2KTA0      Stephen J. Duffey   \n",
       "29998  26001  103538  B002Z08RIA  A39PGI6IGM5Y2A                    Lee   \n",
       "29999  29999   80909  B0029JW9JU  A26ZK8QFWXN859    dalepres \"dalepres\"   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                         0                       1      0  1330646400   \n",
       "1                         0                       3      0  1336953600   \n",
       "2                         1                       1      0  1327449600   \n",
       "3                         2                       7      0  1154995200   \n",
       "4                         0                       3      0  1340582400   \n",
       "...                     ...                     ...    ...         ...   \n",
       "29995                     0                       0      4  1350691200   \n",
       "29996                     1                       1      4  1231891200   \n",
       "29997                     1                       2      4  1308441600   \n",
       "29998                     0                       0      4  1344211200   \n",
       "29999                    11                      11      4  1301184000   \n",
       "\n",
       "                                  Summary  \\\n",
       "0                                   Gross   \n",
       "1                            Don't bother   \n",
       "2              Terribly false advertising   \n",
       "3                        Just plain awful   \n",
       "4                    Coffee Lovers Beware   \n",
       "...                                   ...   \n",
       "29995              Great Tea, Great Price   \n",
       "29996  This is truly the Best Giardiniera   \n",
       "29997                  Wonderful dog food   \n",
       "29998             Excellent coconut juice   \n",
       "29999       I've eaten these for 15 years   \n",
       "\n",
       "                                                    Text  \n",
       "0      I love Italian wedding and Campbell's own regu...  \n",
       "1      I was seriously disappointed with this product...  \n",
       "2      I had my hopes for this bar. The box reads \"Fr...  \n",
       "3      I can at least tolerate most foods, but these ...  \n",
       "4      I purchased this coffee on sale at my local Vo...  \n",
       "...                                                  ...  \n",
       "29995  Tea comes in bags inside a large foil lined po...  \n",
       "29996  If you can't take spicey, then don't buy the H...  \n",
       "29997  I used to buy my dog (5 year old Corgi) the re...  \n",
       "29998  I'm an avid coconut water drinker - I've tried...  \n",
       "29999  I have been eating these pepperonis for 15 yea...  \n",
       "\n",
       "[30000 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload and change the size\n",
    "appended_data = []\n",
    "for i in range(1,6):\n",
    "    temp = []\n",
    "    temp = amazon_reviews[amazon_reviews.Score ==i ].sample(6000)\n",
    "    appended_data.append(temp)\n",
    "    \n",
    "df = pd.concat(appended_data,  ignore_index=True)\n",
    "\n",
    "df['Score'].replace({1: 0, \n",
    "                     2: 1, \n",
    "                     3:2, \n",
    "                     4:3, \n",
    "                     5:4}, inplace=True)\n",
    "\n",
    "df.sort_values(by='Score', ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition\n",
    "train, test = train_test_split(df,\n",
    "                               test_size=0.2,\n",
    "                               random_state=42,\n",
    "                               stratify = df['Score']\n",
    "                              )\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train['Text'])\n",
    "\n",
    "# Train dataset\n",
    "train_sequences = tokenizer.texts_to_sequences(train['Text'])\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=116)\n",
    "\n",
    "# Test dataset\n",
    "test_sequences = tokenizer.texts_to_sequences(test['Text'])\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=116)\n",
    "\n",
    "labels = train['Score']\n",
    "labels = labels.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 1.3734 - accuracy: 0.3924 - val_loss: 1.2317 - val_accuracy: 0.4640\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 1.1384 - accuracy: 0.5181 - val_loss: 1.2231 - val_accuracy: 0.4785\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.9983 - accuracy: 0.5920 - val_loss: 1.2216 - val_accuracy: 0.5023\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.8514 - accuracy: 0.6609 - val_loss: 1.3121 - val_accuracy: 0.4840\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.7127 - accuracy: 0.7276 - val_loss: 1.4432 - val_accuracy: 0.4833\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.5771 - accuracy: 0.7865 - val_loss: 1.5761 - val_accuracy: 0.4894\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 23s 38ms/step - loss: 0.4607 - accuracy: 0.8380 - val_loss: 1.7451 - val_accuracy: 0.4865\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.3614 - accuracy: 0.8768 - val_loss: 1.9463 - val_accuracy: 0.4792\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 23s 38ms/step - loss: 0.2753 - accuracy: 0.9087 - val_loss: 2.1387 - val_accuracy: 0.4733\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 23s 38ms/step - loss: 0.2022 - accuracy: 0.9381 - val_loss: 2.3434 - val_accuracy: 0.4710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be631f2888>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx6 = Sequential()\n",
    "model_ecx6.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx6.add(Dense(128, activation='relu'))\n",
    "model_ecx6.add(Dense(128, activation='relu'))\n",
    "model_ecx6.add(GlobalMaxPooling1D())\n",
    "model_ecx6.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx6.compile(loss='sparse_categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "model_ecx6.fit(train_sequences, labels, validation_split=0.2, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization through early stopping\n",
    "\n",
    "We have consistently seen that our neural network overfits at around the third epoch. Hence, another form of regularization is to end training early if validation loss starts increasing. (This is similar to the validation curves we used when constructing classification models.) Although the network will not have found an optimal function in the training data, the looser function that it has found will likely be more generalizable.\n",
    "\n",
    "You can do this manually by inspecting the data as we have done above and modifying the `epochs` argument in `fit()`, but Keras also allows you to easily do this automatically via an [`EarlyStopping` callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Experiment with the `EarlyStopping` callback and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 23s 39ms/step - loss: 1.3752 - accuracy: 0.3897 - val_loss: 1.2440 - val_accuracy: 0.4658\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 23s 38ms/step - loss: 1.1312 - accuracy: 0.5202 - val_loss: 1.2304 - val_accuracy: 0.4794\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 23s 38ms/step - loss: 0.9867 - accuracy: 0.5924 - val_loss: 1.2349 - val_accuracy: 0.4931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be03392648>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx7 = Sequential()\n",
    "model_ecx7.add(Embedding(20000, 128, input_length=116))\n",
    "model_ecx7.add(Dense(128, activation='relu'))\n",
    "model_ecx7.add(Dense(128, activation='relu'))\n",
    "model_ecx7.add(GlobalMaxPooling1D())\n",
    "model_ecx7.add(Dense(5, activation='sigmoid'))\n",
    "model_ecx7.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ecx7.fit(train_sequences, labels, validation_split=0.2, epochs=10,\n",
    " callbacks=[EarlyStopping(monitor='val_loss', mode='min')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in most previous cases, we used *three* splits of our data instead of two. All of our model tuning has been done on the validation set, and we have not even touched the test set that we split off right at the start.\n",
    "\n",
    "For experiments, it's very important that your model is only run **once** on your test set. As there is so much randomness at play, it's vital to not \"cherry-pick\" the best results, so optimize as much as you want on the validation set, but keep the test set until the end and all official results should be based on the single run of the test set (or whatever configuration was decided *before the experiment started*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "20_min"
    ]
   },
   "source": [
    "### Exercise 8:\n",
    "\n",
    "Let's take the model configuration that resulted in the highest validation accuracy and use that one as our final model. Evaluate this configuration on how well it performs on the test set, and furthermore diagnose *what kinds of mistakes it makes*. Explain whether these mistakes are expected or not, and print some of these poorly classified reviews. Given the mistakes the model made, how would you then go back and try to improve the model or optimize the tuning steps?\n",
    "\n",
    "**Hint:** You can use the [`predict_classes`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict_classes) method on your model to get the most probable class directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-aabed8cefd05>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, ..., 4, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ecx7.predict_classes(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.478\n"
     ]
    }
   ],
   "source": [
    "# Save results into test data frame\n",
    "pred = model_ecx7.predict_classes(test_sequences)\n",
    "print(accuracy_score(test['Score'], pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[672, 262, 136,  53,  77],\n",
       "       [257, 429, 329, 110,  75],\n",
       "       [ 85, 225, 487, 275, 128],\n",
       "       [ 57,  75, 255, 443, 370],\n",
       "       [ 41,  37,  80, 205, 837]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test['Score'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.56      0.58      1200\n",
      "           1       0.42      0.36      0.39      1200\n",
      "           2       0.38      0.41      0.39      1200\n",
      "           3       0.41      0.37      0.39      1200\n",
      "           4       0.56      0.70      0.62      1200\n",
      "\n",
      "    accuracy                           0.48      6000\n",
      "   macro avg       0.47      0.48      0.47      6000\n",
      "weighted avg       0.47      0.48      0.47      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test['Score'], pred))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative would be to review other network architectures that are more robust and suitable for the problem, you can also perform a pre-cleaning of the data as was done in EC4 and review how in conjunction with different neural network architectures improves or not the performance of the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have seen from this that there is no one-size-fits-all method when creating model architectures or tuning parameters. Often times, copious experimentation is needed, and even then it can be difficult to get significantly better results than a baseline model or even really diagnose what is going wrong under the hood (since neural networks are so \"black-box\"). In many cases, the quantity and quality of the data itself is far more important than the architecture of the network for getting good results."
   ]
  }
 ],
 "metadata": {
  "c1_recart": "7.6.0-57c20131aabc1dc2a8c675852d80a7da",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
